from lib.correction_functions import get_sectoral_profiles, clean_names
from lib.spatial_functions import *
from lib.input_maps import *


def generate_sites_from_shapefile(paths, param):
    """
    This function reads the geodataframe of the sites and extracts their respective names and areas, as well as the coordinates
    of their centroids. It adds assumptions about other attributes and saves the output in a CSV file.
    
    :param paths: Dictionary including the paths to the rasters *LAND*, *EEZ*, and to the output *sites_sub*.
    :type paths: dict
    :param param: Dictionary including the geodataframe of regions, and parameters defining the resolution, the coordinates of the scope, and the georeference dictionary.
    :type param: dict
    
    :return: The CSV file with the sites is saved directly in the desired path, along with the corresponding metadata in a JSON file.
    """
    timecheck("Start")

    # Initialize region masking parameters
    Crd_all = param["Crd_all"]
    GeoRef = param["GeoRef"]
    res_desired = param["res_desired"]
    nRegions = param["nRegions_sub"]
    regions_shp = param["regions_sub"]

    # Initialize dataframe
    regions = pd.DataFrame(
        0,
        index=range(0, nRegions),
        columns=[
            "Name",
            "Index_shapefile",
            "Area_m2",
            "Longitude",
            "Latitude",
            "slacknode",
            "syncarea",
            "ctrarea",
            "primpos",
            "primneg",
            "secpos",
            "secneg",
            "terpos",
            "terneg",
        ],
    )

    # Read masks
    with rasterio.open(paths["LAND"]) as src:
        A_land = src.read(1)
        A_land = np.flipud(A_land).astype(int)
    with rasterio.open(paths["EEZ"]) as src:
        A_sea = src.read(1)
        A_sea = np.flipud(A_sea).astype(int)

    status = 0
    for reg in range(0, nRegions):
        # Display Progress
        status += 1
        display_progress("Generating sites ", (nRegions, status))

        # Compute region_mask
        A_region_extended = calc_region(regions_shp.loc[reg], Crd_all, res_desired, GeoRef)

        # Get name of region
        if np.nansum(A_region_extended * A_land) > np.nansum(A_region_extended * A_sea):
            regions.loc[reg, "Name"] = regions_shp.loc[reg]["NAME_SHORT"]
        else:
            regions.loc[reg, "Name"] = regions_shp.loc[reg]["NAME_SHORT"] + "_offshore"

        # Calculate longitude and latitude of centroids
        regions.loc[reg, "Longitude"] = regions_shp.geometry.centroid.loc[reg].x
        regions.loc[reg, "Latitude"] = regions_shp.geometry.centroid.loc[reg].y

    # Calculate area using Lambert Cylindrical Equal Area EPSG:9835
    regions_shp = regions_shp.to_crs("+proj=cea")
    regions["Area_m2"] = regions_shp.geometry.area
    regions_shp = regions_shp.to_crs({"init": "epsg:4326"})

    # Get original index in shapefile
    regions["Index_shapefile"] = regions_shp["original_index"]

    # Assign slack node
    regions["slacknode"] = 0
    regions.loc[0, "slacknode"] = 1

    # Define synchronous areas and control areas
    regions["syncharea"] = 1
    regions["ctrarea"] = 1

    # Define reserves
    regions["primpos"] = 0
    regions["primneg"] = 0
    regions["secpos"] = 0
    regions["secneg"] = 0
    regions["terpos"] = 0
    regions["terneg"] = 0

    # Export model-independent list of regions
    regions.to_csv(paths["sites_sub"], index=False, sep=";", decimal=",")
    create_json(
        paths["sites_sub"],
        param,
        ["region_name", "subregions_name", "Crd_all", "res_desired", "GeoRef"],
        paths,
        ["LAND", "EEZ", "spatial_scope", "subregions"],
    )
    timecheck("End")


def generate_intermittent_supply_timeseries(paths, param):
    """
    This function reads the time series .csv files generated by the renewable-timeseries tool, validate that
    the desired and available data matches, format the data into a neutral format and saves it into a .csv file.

    :param paths: Dictionary including the paths to the generated TS
    :type paths: dict
    :param param: Dictionary including the potential time-series parameters
    :type param: dict

    :return: The time series for each region, and for all technology, mode and combo desired are saved directly in the given path, along with the metadata in a JSON file.
    :rtype: None
    :raise Missing TS: The time series file for desired combo is missing.
    :raise Missing mode: The desired mode is missing from the time series file.
    :raise Sub-regions missing: Some sub-regions are not present in the time series file, and will be left zero in the output .csv file.
    """
    timecheck('Start')
    Timeseries = None

    # Prepare empty dataframe
    # Loop over the technologies understudy
    for tech in param["ren_potential"].keys():
        for mode in param["ren_potential"][tech]:
            # Check 1: TS file exist
            if os.path.isfile(paths["TS_ren"][tech]):
                TS = pd.read_csv(paths["TS_ren"][tech], sep=';', decimal=',', index_col=[0])
            else:
                warn("No time series found for " + tech + " under path: " + paths["TS_ren"][tech], UserWarning)
                continue

            # Retrieve available modes and sub-regions from TS file
            avail_modes = []
            avail_subregions = []
            for col in list(TS.columns):
                # Split string
                split = re.split("_", col)
                avail_modes = avail_modes + [split[-1]]
                avail_subregions = avail_subregions + [split[0]]
                combo_name = split[2]

            # Remove duplicates
            avail_modes = list(set(avail_modes))
            avail_subregions = list(set(avail_subregions))

            # Check 2: Mode available in TS file
            if mode not in avail_modes:
                warn("Desired mode " + mode + " for technology " + tech + " not found in time series file", UserWarning)
                continue

            # Check 3: All desired regions available in TS file
            sub_regions = list(param["regions_sub"]["NAME_SHORT"])
            missing = list([item for item in sub_regions if item not in avail_subregions])
            if missing:
                warn("For technology " + tech + ", the following subregions are missing from the time series file: \n" + str(missing) + ". The potential time series will be set to zero.", UserWarning)

            # Prepare tech dataframe
            if mode in ["all", "ALL", "All"]:
                suffix = ""
            else:
                suffix =  "_" + mode
            TS_tech = pd.DataFrame(None, range(0, 8760), columns=list([sub + '.' + tech + suffix for sub in sub_regions]))

            # Loop over regions
            for reg in sub_regions:
                if reg in avail_subregions:
                    entry = list(map(str, [reg, tech, combo_name, mode]))
                    TS_tech[reg + "." + tech + suffix] = TS["_".join(entry)]

            # Replace nan values with zeros
            TS_tech.fillna(value=0, inplace=True)
            TS_tech.index = range(1, 8761)
            if Timeseries is None:
                Timeseries = TS_tech.copy()
            else:
                Timeseries = pd.concat([Timeseries, TS_tech], axis=1)

    Timeseries.to_csv(paths["potential_ren"], sep=';', decimal=',')
    print("File Saved: " + paths["potential_ren"])
    create_json(paths["potential_ren"], param, ["region_name", "subregions_name", "technology", "ren_potential"], paths, ["TS_ren"])
    timecheck('End')


def generate_load_timeseries(paths, param):
    """
    This function reads the normalized sectoral standard load profiles, and the cleaned load time series for the countries in the scope.
    On one hand, it splits the time series into sectoral time series for each country. On the other hand, it determines the time series for each pixel of land use
    and for each person in the country, by assuming a relationship between sectors and land use types / population size. Finally, it aggregates the time series of the
    pixels that lie in the same subregions to obtain the time series for each desired subregion.
    
    :param paths: Dictionary containing the paths to the cleaned input, to the intermediate files and to the outputs.
    :type paths: dict
    :param param: Dictionary containing assumptions about the load, as well as the geodataframes for the countries and the subregions.
    :type param: dict
    
    :return: All the outputs are saved in CSV or SHP files in the defined paths, along with their metadata in JSON files.
    :rtype: None
    """

    timecheck("Start")

    # Sector land use allocation
    sector_lu = pd.read_csv(paths["assumptions_landuse"], index_col=0, sep=";", decimal=",")
    shared_sectors = set(sector_lu.columns).intersection(set(param["load"]["sectors"]))
    sector_lu = sector_lu[sorted(list(shared_sectors))]
    shared_sectors.add("RES")
    if not shared_sectors == set(param["load"]["sectors"]):
        warn(
            "The following sectors are not included in " + paths["assumptions_landuse"] + ": " + str(set(param["load"]["sectors"]) - shared_sectors),
            UserWarning,
        )

    landuse_types = [str(i) for i in sector_lu.index]
    param["landuse_types"] = landuse_types
    # Normalize the land use coefficients found in the assumptions table over each sector
    sector_lu = sector_lu.transpose().div(np.repeat(sector_lu.sum(axis=0)[:, None], len(sector_lu), axis=1))
    sec = [str(i) for i in sector_lu.index]

    # Share of sectors in electricity demand
    sec_share = pd.read_csv(paths["sector_shares_clean"], index_col=0, sep=";", decimal=",")

    # Create landuse and population maps, if they do not exist already
    if not os.path.exists(paths["LU"]):
        generate_landuse(paths, param)
    if not os.path.exists(paths["POP"]):
        generate_population(paths, param)

    # Count pixels of each land use type and create weighting factors for each country
    if not os.path.exists(paths["stats_countries"]):
        df = zonal_stats(param["regions_land"], {"Population": paths["POP"], "Landuse": paths["LU"]}, param)
        stat = param["regions_land"][["GID_0"]].rename(columns={"GID_0": "Country"}).join(df).set_index("Country")
        stat.to_csv(paths["stats_countries"], sep=";", decimal=",", index=True)
        create_json(paths["stats_countries"], param, ["region_name", "year", "load", "landuse_types"], paths, ["spatial_scope", "LU", "POP"])
    else:
        stat = pd.read_csv(paths["stats_countries"], sep=";", decimal=",", index_col=0)

    # Weighting by sector
    for s in sec:
        stat.loc[:, s] = np.dot(stat.loc[:, landuse_types], sector_lu.loc[s])

    if not (os.path.isfile(paths["df_sector"]) and os.path.isfile(paths["load_sector"]) and os.path.isfile(paths["load_landuse"])):

        # Get dataframe with cleaned timeseries for countries
        df_load_countries = pd.read_csv(paths["load_ts_clean"], sep=";", decimal=",")
        countries = param["regions_land"].rename(columns={"GID_0": "Country"})

        # Get sectoral normalized profiles
        profiles = get_sectoral_profiles(paths, param)

        # Prepare an empty table of the hourly load for the five sectors in each countries.
        df_sectors = pd.DataFrame(
            0,
            index=df_load_countries.index,
            columns=pd.MultiIndex.from_product([df_load_countries.columns.tolist(), param["load"]["sectors"]], names=["Country", "Sector"]),
        )

        # Copy the load profiles for each sector in the columns of each country, and multiply each sector by the share
        # defined in 'sec_share'. Note that at the moment the values are the same for all countries
        for c in df_load_countries.columns:
            for s in param["load"]["sectors"]:
                try:
                    df_sectors.loc[:, (c, s)] = profiles[s] * sec_share.loc[c, s]
                except KeyError:
                    df_sectors.loc[:, (c, s)] = profiles[s] * sec_share.loc[param["load"]["default_sec_shares"], s]

        # Normalize the load profiles over all sectors by the hour so that the sum of the loads of all sectors = 1
        # for each hour, then multiply with the actual hourly loads for each country
        df_scaling = df_sectors.groupby(level=0, axis=1).sum()
        for c in df_load_countries.columns:
            for s in sec + ["RES"]:
                df_sectors.loc[:, (c, s)] = df_sectors.loc[:, (c, s)] / df_scaling[c] * df_load_countries[c]

        # Calculate the yearly load per sector and country
        load_sector = df_sectors.sum(axis=0).rename("Load in MWh")

        # Prepare dataframe load_landuse, which calculates the hourly load for each land use unit in each country
        rows = landuse_types.copy()
        rows.append("RES")
        countries = sorted(list(set(stat.index.tolist()).intersection(set(df_load_countries.columns))))
        m_index = pd.MultiIndex.from_product([countries, rows], names=["Country", "Land use"])
        load_landuse = pd.DataFrame(0, index=m_index, columns=df_sectors.index)

        status = 0
        length = len(countries) * len(landuse_types) * len(sec)
        display_progress("Computing regions load", (length, status))
        for c in countries:  # Countries
            load_landuse.loc[c, "RES"] = load_landuse.loc[c, "RES"] + df_sectors[(c, "RES")] / stat.loc[c, "RES"]
            for lu in landuse_types:  # Land use types
                for s in sec:  # other sectors
                    load_landuse.loc[c, lu] = load_landuse.loc[c, lu] + sector_lu.loc[s, int(lu)] * df_sectors[(c, s)] / stat.loc[c, s]
                    status = status + 1
                    display_progress("Computing regions load", (length, status))

        # Save the data into HDF5 files for faster execution
        df_sectors.to_csv(paths["df_sector"], sep=";", decimal=",", index=False, header=True)
        create_json(paths["df_sector"], param, ["region_name", "year", "load"], paths, ["spatial_scope", "load_ts_clean"])
        print("Dataframe with time series for each country and sector saved: " + paths["df_sector"])
        load_sector.to_csv(paths["load_sector"], sep=";", decimal=",", index=True, header=True)
        create_json(paths["load_sector"], param, ["region_name", "year", "load"], paths, ["spatial_scope", "load_ts_clean"])
        print("Dataframe with yearly demand for each country and sector saved: " + paths["load_sector"])
        load_landuse.to_csv(paths["load_landuse"], sep=";", decimal=",", index=True)
        create_json(paths["load_landuse"], param, ["region_name", "year", "load", "landuse_types"], paths, ["spatial_scope", "load_ts_clean"])
        print("Dataframe with time series for each land use pixel saved: " + paths["load_landuse"])

    # Read CSV files
    df_sectors = pd.read_csv(paths["df_sector"], sep=";", decimal=",", header=[0, 1])
    load_sector = pd.read_csv(paths["load_sector"], sep=";", decimal=",", index_col=[0, 1])["Load in MWh"]
    load_landuse = pd.read_csv(paths["load_landuse"], sep=";", decimal=",", index_col=[0, 1])

    # Split subregions into country parts
    # (a subregion can overlap with many countries, but a country part belongs to only one country)
    reg_intersection = intersection_subregions_countries(paths, param)

    # Count number of pixels for each country part
    if not os.path.exists(paths["stats_country_parts"]):
        df = zonal_stats(reg_intersection, {"Population": paths["POP"], "Landuse": paths["LU"]}, param)
        stat_sub = reg_intersection[["NAME_SHORT"]].rename(columns={"NAME_SHORT": "Country_part"}).join(df).set_index("Country_part")
        stat_sub.to_csv(paths["stats_country_parts"], sep=";", decimal=",", index=True)
        create_json(
            paths["stats_country_parts"],
            param,
            ["region_name", "subregions_name", "landuse_types"],
            paths,
            ["spatial_scope", "LU", "POP", "Countries", "subregions"],
        )
    else:
        stat_sub = pd.read_csv(paths["stats_country_parts"], sep=";", decimal=",", index_col=0)

    # Add attributes for country/region
    stat_sub["Region"] = 0
    stat_sub["Country"] = 0
    for i in stat_sub.index:
        stat_sub.loc[i, ["Region", "Country"]] = i.split("_")
        if stat_sub.loc[i, "Country"] not in list(df_sectors.columns.get_level_values(0).unique()):
            stat_sub.drop(index=i, inplace=True)

    # Prepare dataframe to save the hourly load in each country part
    load_country_part = pd.DataFrame(0, index=stat_sub.index, columns=df_sectors.index.tolist() + ["Region", "Country"])
    load_country_part[["Region", "Country"]] = stat_sub[["Region", "Country"]]

    # Calculate the hourly load for each subregion
    status = 0
    length = len(load_country_part.index) * len(landuse_types)
    display_progress("Computing sub regions load:", (length, status))
    for cp in load_country_part.index:
        c = load_country_part.loc[cp, "Country"]
        # For residential:
        load_country_part.loc[cp, df_sectors.index.tolist()] = (
            load_country_part.loc[cp, df_sectors.index.tolist()] + stat_sub.loc[cp, "RES"] * load_landuse.loc[c, "RES"].to_numpy()
        )
        for lu in landuse_types:
            load_country_part.loc[cp, df_sectors.index.tolist()] = (
                load_country_part.loc[cp, df_sectors.index.tolist()] + stat_sub.loc[cp, lu] * load_landuse.loc[c, lu].to_numpy()
            )
            # Show progress
            status = status + 1
            display_progress("Computing load in country parts", (length, status))

    # Aggregate into subregions
    load_regions = load_country_part.groupby(["Region", "Country"]).sum()
    load_regions.reset_index(inplace=True)
    load_regions = load_regions.groupby(["Region"]).sum().T

    # Output
    load_regions.to_csv(paths["load_regions"], sep=";", decimal=",", index=True)
    create_json(
        paths["load_regions"],
        param,
        ["region_name", "subregions_name", "load", "landuse_types"],
        paths,
        ["spatial_scope", "LU", "POP", "Countries", "subregions"],
    )
    print("File saved: " + paths["load_regions"])

    timecheck("End")


def generate_transmission(paths, param):
    """
    This function reads the cleaned grid data and the shapefile of the subregions. It first determines the names of the regions
    connected by each line, *Region_start* and *Region_end*, and only keeps those between two different subregions. Then it estimates
    the length between the centroids of the regions and uses it to estimate the efficiency of the lines and their costs. Finally,
    it completes the missing attributes with general assumptions and saves the result in a CSV file.
    
    :param paths: Dictionary including the paths to *grid_cleaned*, *sites_sub*, *subregions*, *dict_lines_costs*, and the output *grid_completed*.
    :type param: dict
    :param param: Dictionary including the geodataframe of the subregions and grid-related assumptions.
    :type param: dict
    
    :return: The CSV file with the completed transmission data is saved directly in the desired path, along with its metadata in a JSON file.
    :rtype: None
    """
    timecheck("Start")

    # Read the geodataframe of subregions
    subregions = param["regions_sub"][["NAME_SHORT", "geometry"]]

    # Read the cleaned GridKit dataset
    grid_cleaned = pd.read_csv(paths["grid_cleaned"], header=0, sep=";", decimal=",")

    # Create point geometries
    grid_cleaned["V1"] = list(zip(grid_cleaned.V1_long, grid_cleaned.V1_lat))
    grid_cleaned["V1"] = grid_cleaned["V1"].apply(Point)
    grid_cleaned["V2"] = list(zip(grid_cleaned.V2_long, grid_cleaned.V2_lat))
    grid_cleaned["V2"] = grid_cleaned["V2"].apply(Point)

    # Create a dataframe for the start regions
    Region_start = gpd.GeoDataFrame(grid_cleaned[["l_id", "V1"]], geometry="V1", crs={"init": "epsg:4326"}).rename(columns={"V1": "geometry"})
    Region_start.crs = subregions.crs
    Region_start = gpd.sjoin(Region_start, subregions, how="left", op="intersects")[["NAME_SHORT"]].rename(columns={"NAME_SHORT": "Region_start"})

    # Create a dataframe for the end regions
    Region_end = gpd.GeoDataFrame(grid_cleaned[["l_id", "V2"]], geometry="V2", crs={"init": "epsg:4326"}).rename(columns={"V2": "geometry"})
    Region_end.crs = subregions.crs
    Region_end = gpd.sjoin(Region_end, subregions, how="left", op="intersects")[["NAME_SHORT"]].rename(columns={"NAME_SHORT": "Region_end"})

    # Join dataframes
    grid_regions = grid_cleaned.drop(["V1", "V2"], axis=1).join([Region_start, Region_end])

    intra = len(grid_regions.loc[(grid_regions["Region_start"] == grid_regions["Region_end"]) & ~(grid_regions["Region_start"].isnull())])
    extra = len(grid_regions.loc[grid_regions["Region_start"].isnull() | grid_regions["Region_end"].isnull()])
    inter = len(grid_regions) - intra - extra

    # Show numbers of intraregional, interregional and extraregional lines
    print("\nLinetypes : ")
    print((("intraregional", intra), ("interregional", inter), ("extraregional", extra)))

    # Remove intraregional and extraregional lines
    lines_concatenated = grid_regions.loc[
        (grid_regions["Region_start"] != grid_regions["Region_end"]) & ~(grid_regions["Region_start"].isnull() | grid_regions["Region_end"].isnull())
    ].copy()

    # Sort alphabetically and reindex
    lines_reversed = reverse_lines(lines_concatenated)
    lines_reversed.sort_values(["Region_start", "Region_end", "tr_type"], inplace=True)
    lines = lines_reversed.set_index(["Region_start", "Region_end", "tr_type"]).reset_index()
    lines.drop(["V1_long", "V1_lat", "V2_long", "V2_lat", "l_id"], axis=1, inplace=True)

    # Aggregate lines starting and ending in the same regions
    lines_grouped = lines.groupby(["Region_start", "Region_end", "tr_type"]).sum()

    # Reindex and rename columns
    lines_final = lines_grouped.reset_index().rename(columns={"Region_start": "Site In", "Region_end": "Site Out", "Capacity_MVA": "cap-up-therm"})
    lines_final["reactance"] = 1 / lines_final["Y_mho_ref_380kV"]

    # Create a dataframe to store all the possible combinations of pairs of 1st order neighbors
    df = pd.DataFrame(columns=["Site In", "Site Out"])
    zones = pd.read_csv(paths["sites_sub"], index_col=0, decimal=",", sep=";")
    weights = ps.lib.weights.Queen.from_shapefile(paths["subregions"])
    for z in range(len(zones)):
        for n in weights.neighbors[z]:
            if zones.iloc[z].name < zones.iloc[n].name:
                df = df.append(pd.DataFrame([[zones.iloc[z].name, zones.iloc[n].name]], columns=["Site In", "Site Out"]), ignore_index=True)

    # Join that dataframe with existing lines
    df["tr_type"] = "AC_OHL"
    df.set_index(["Site In", "Site Out", "tr_type"], inplace=True)
    df_joined = df.join(lines_final.set_index(["Site In", "Site Out", "tr_type"]), how="outer")

    # Fill empty values for capacity (inexistent lines)
    df_joined["cap-up-therm"].fillna(0, inplace=True)

    # Calculate length of lines based on distance between centroids
    df_joined.reset_index(drop=False, inplace=True)
    df_joined = df_joined.join(zones[["Longitude", "Latitude"]], on="Site In", rsuffix="_1", how="inner")
    df_joined = df_joined.join(zones[["Longitude", "Latitude"]], on="Site Out", rsuffix="_2", how="inner")
    df_joined["length"] = [
        distance.distance(
            tuple(df_joined.loc[i, ["Latitude", "Longitude"]].astype(float)), tuple(df_joined.loc[i, ["Latitude_2", "Longitude_2"]].astype(float))
        ).km
        for i in df_joined.index
    ]
    df_joined.drop(["Longitude", "Latitude", "Longitude_2", "Latitude_2"], axis=1, inplace=True)

    # Calculate efficiency
    dict_eff = param["grid"]["efficiency"]
    df_joined.reset_index(drop=True, inplace=True)
    df_joined["eff"] = 1
    for ind in df_joined.index:
        df_joined.loc[ind, "eff"] = dict_eff[df_joined.loc[ind, "tr_type"]] ** (df_joined.loc[ind, "length"] / 1000)

    # Calculate costs
    dict_lines_costs = pd.read_csv(paths["dict_lines_costs"], sep=";", decimal=",")
    dict_lines_costs.loc[dict_lines_costs["length_limit_km"] == "inf", "length_limit_km"] = np.inf
    df_joined["inv-cost"] = 0
    df_joined["fix-cost"] = 0
    df_joined["var-cost"] = 0
    for ind in df_joined.index:
        filter = (dict_lines_costs["tr_type"] == df_joined.loc[ind, "tr_type"]) & (dict_lines_costs["length_limit_km"] > df_joined.loc[ind, "length"])
        dict_costs = dict_lines_costs.loc[filter].sort_values(by=["length_limit_km"], axis=0).head(1)
        df_joined.loc[ind, "inv-cost"] = float(dict_costs["inv-cost-length"]) * df_joined.loc[ind, "length"] + float(dict_costs["inv-cost-fix"])
        df_joined.loc[ind, "fix-cost"] = float(dict_costs["fix-cost-length"]) * df_joined.loc[ind, "length"]

    # Add attributes
    df_completed = df_joined.copy()
    df_completed["Commodity"] = "Elec"
    df_completed["inst-cap"] = df_completed["cap-up-therm"]
    df_completed["act-lo"] = 0
    df_completed["act-up"] = 1
    df_completed["angle-up"] = 45
    df_completed["PSTmax"] = 0
    df_completed["cap-lo"] = 0
    df_completed["cap-up"] = df_completed["inst-cap"]
    df_completed["idx"] = df_completed.index + 1
    df_completed["wacc"] = param["grid"]["wacc"]
    df_completed["depreciation"] = param["grid"]["depreciation"]

    # Ouput
    df_completed.to_csv(paths["grid_completed"], sep=";", decimal=",", index=False)
    print("File saved: " + paths["grid_completed"])
    create_json(paths["grid_completed"], param, ["grid"], paths, ["transmission_lines", "grid_cleaned", "subregions", "dict_lines_costs"])

    timecheck("End")


def generate_commodity(paths, param):
    """ documentation """
    timecheck("Start")
    
    # Read assumptions related to the flows and commodities
    assumptions_flows = pd.read_csv(paths["assumptions_flows"], sep=";", decimal=",")
    assumptions_com = pd.read_csv(paths["assumptions_commodities"], sep=";", decimal=",")
    
    # Only use the assumptions of that particular year
    assumptions_flows = assumptions_flows.loc[assumptions_flows["year"] == param["model_year"]]
    assumptions_com = assumptions_com.loc[assumptions_com["year"] == param["model_year"]]
    assumptions_com.drop(columns=["year"], inplace=True)
    
    # Only use flows and commodities that are needed by the user
    assumptions_flows = assumptions_flows.loc[assumptions_flows["Process/Storage"].isin(param["technology"]["Process"])]
    assumptions_com = assumptions_com.loc[assumptions_com["Commodity"].isin(assumptions_flows["Commodity"].unique())]
    
    # Clean assumptions about commodities
    assumptions_com.set_index(["Commodity"], inplace=True)
    assumptions_com.replace(to_replace="inf", value=np.inf, inplace=True)
    
    # Obtain combinations of sites and commodities
    com_list = list(assumptions_com.index.unique())
    site_list = list(pd.read_csv(paths["sites_sub"], sep=";", decimal=",", index_col=0).index.unique())
    df_com = pd.DataFrame(index = pd.MultiIndex.from_product([site_list, com_list], names=["Site", "Commodity"])) \
               .reset_index() \
               .join(assumptions_com, on=["Commodity"], how="left") \
               .set_index(["Site"])
               
    # Read the CSV containing the load time series
    load = pd.read_csv(paths["load_regions"], sep=";", decimal=",", index_col=0)
    
    # Correct the annual load
    df_com.loc[df_com["Commodity"]=="Elec", "annual"] = load.sum(axis=0)

    df_com.to_csv(paths["commodities_regions"], index=True, sep=";", decimal=",")
    print("File saved: " + paths["commodities_regions"])
    create_json(paths["commodities_regions"], param, ["model_year", "technology"], paths, ["assumptions_flows", "assumptions_commodities", "sites_sub", "load_regions"])

    timecheck("End")


def generate_processes(paths, param):
    """ documentation """
    timecheck("Start")
    
    # Read assumptions related to the processes and flows
    assumptions_pro = pd.read_csv(paths["assumptions_processes"], sep=";", decimal=",")
    assumptions_flows = pd.read_csv(paths["assumptions_flows"], sep=";", decimal=",")
    
    # Only use the assumptions of that particular year
    assumptions_pro = assumptions_pro.loc[assumptions_pro["year"] == param["model_year"]]
    assumptions_flows = assumptions_flows.loc[assumptions_flows["year"] == param["model_year"]]
    assumptions_pro.drop(columns=["year"], inplace=True)
    assumptions_flows.drop(columns=["year"], inplace=True)
    
    # Only use processes and flows that are needed by the user
    assumptions_pro = assumptions_pro.loc[assumptions_pro["Process"].isin(param["technology"]["Process"])]
    assumptions_flows = assumptions_flows.loc[assumptions_flows["Process/Storage"].isin(param["technology"]["Process"])]
    
    # Read shapefile of processes and storage
    process_shp = gpd.read_file(paths["process_cleaned"])
    
    # Filter out process types not needed by the user
    process_shp = process_shp.loc[process_shp["Type"].isin(param["technology"]["Process"])]
    print("Number of power plants which belong to the modeled technologies: ", len(process_shp), "- installed capacity: ", process_shp["inst-cap"].sum())
    
    # Join shapefile with process assumptions
    assumptions_pro.set_index(["Process"], inplace=True)
    assumptions_pro.replace(to_replace="inf", value=np.inf, inplace=True)
    process_shp = process_shp.join(assumptions_pro, on=["Type"], how="left")
    
    # Filter out processes that have exceeded their lifetime
    if param["model_year"] > param["year"]:
        process_shp = process_shp.loc[(process_shp["lifetime"] + process_shp["Year"]) >= param["model_year"]]
        print("Number of power plants that have not reached their end of life: ", len(process_shp), "- installed capacity: ", process_shp["inst-cap"].sum())

    # Associate each power plant to a subregion
    process_shp = get_sites(process_shp, param)
    print("Number of power plants that lie in the modeled subregions: ", len(process_shp), "- installed capacity: ", process_shp["inst-cap"].sum())
    
    # Associate each power plant to a cohort
    process_shp["Cohort"] = (process_shp["Year"] // param["process"]["cohorts"]) * param["process"]["cohorts"]
    
    # Group must-run power plants with similar characteristics
    filter = process_shp.loc[process_shp["on-off"] == 0].index
    must_run = process_shp.loc[filter, ["Type", "inst-cap", "Cohort", "Site"]]
    must_run = must_run.groupby(["Type", "Cohort", "Site"]) \
                       .sum() \
                       .reset_index() \
                       .join(assumptions_pro, on=["Type"], how="left")
    must_run["Name"] = must_run["Type"] + "_" + must_run["Cohort"].astype(int).astype(str)
    process_shp = process_shp.loc[process_shp["on-off"] == 1] \
                             .append(must_run, ignore_index=True)
    print("Number of power plants after grouping must-run power plants: ", len(process_shp), "- installed capacity: ", process_shp["inst-cap"].sum())

    # Correct cap-up
    process_shp["cap-up"] = process_shp["inst-cap"].astype(float)
    
    # Clean and shorten names
    process_shp["Name"] = process_shp["Name"].apply(clean_names)
    
    # Obtain combinations for possible expansion
    pro_expansion = list(assumptions_pro.loc[assumptions_pro["cap-up"]!=0].index.unique())
    site_expansion = list(pd.read_csv(paths["sites_sub"], sep=";", decimal=",", index_col=0).index.unique())
    site_offshore = [site for site in site_expansion if site.endswith("_offshore")]
    if len(site_offshore) and ("WindOff" in pro_expansion):
        pro_expansion.remove("WindOff")
        site_expansion = [site for site in site_expansion if site not in site_offshore]
        ind_expansion_off = pd.MultiIndex.from_product([site_offshore, ["WindOff"]], names=["Site", "Type"])
    else:
        pro_expansion.remove("WindOff")
        ind_expansion_off = pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=["Site", "Type"])
    df_expansion = pd.DataFrame(index = pd.MultiIndex.from_product([site_expansion, pro_expansion], names=["Site", "Type"]).append(ind_expansion_off)) \
                     .reset_index() \
                     .join(assumptions_pro, on=["Type"], how="left")
    df_expansion["Cohort"] = param["model_year"]
    df_expansion["Name"] = df_expansion["Type"] + "_" + str(param["model_year"])
    df_expansion["inst-cap"] = 0
    process_shp = process_shp.append(df_expansion, ignore_index=True)
    print("Number of power plants after including potential expansion: ", len(process_shp), "- installed capacity: ", process_shp["inst-cap"].sum())
    
    # Derive efficiency and specific CO2 emissions from assumptions_flows
    assumptions_flows.set_index(["Process/Storage"], inplace=True)
    df_eff = assumptions_flows.loc[(assumptions_flows["Direction"]=="Out") & (assumptions_flows["Commodity"]=="Elec"), "ratio"] / assumptions_flows.loc[assumptions_flows["Direction"]=="In", "ratio"]
    df_effmin = assumptions_flows.loc[(assumptions_flows["Direction"]=="Out") & (assumptions_flows["Commodity"]=="Elec"), "ratio-min"] / assumptions_flows.loc[assumptions_flows["Direction"]=="In", "ratio-min"]
    df_co2 = assumptions_flows.loc[(assumptions_flows["Direction"]=="Out") & (assumptions_flows["Commodity"]=="CO2"), "ratio"] / assumptions_flows.loc[assumptions_flows["Direction"]=="In", "ratio"]
    df_co2.fillna(0, inplace=True)
    process_shp["eff"] = process_shp[["Type"]].join(df_eff, on=["Type"], how="left")["ratio"]
    process_shp["effmin"] = process_shp[["Type"]].join(df_effmin, on=["Type"], how="left")["ratio-min"]
    process_shp["cotwo"] = process_shp[["Type"]].join(df_co2, on=["Type"], how="left")["ratio"]      
    
    # Output
    process_shp.to_csv(paths["process_regions"], index=False, sep=";", decimal=",")
    print("File saved: " + paths["process_regions"])
    create_json(paths["process_regions"], param, ["region_name", "subregions_name", "year", "model_year", "technology", "process"], paths, ["assumptions_processes", "assumptions_flows", "process_cleaned"])
    
    timecheck("End")


def generate_storage(paths, param):
    """ documentation """
    timecheck("Start")
    
    # Read assumptions related to the storage and flows
    assumptions_sto = pd.read_csv(paths["assumptions_storage"], sep=";", decimal=",")
    assumptions_flows = pd.read_csv(paths["assumptions_flows"], sep=";", decimal=",")
    
    # Only use the assumptions of that particular year
    assumptions_sto = assumptions_sto.loc[assumptions_sto["year"] == param["model_year"]]
    assumptions_flows = assumptions_flows.loc[assumptions_flows["year"] == param["model_year"]]
    assumptions_sto.drop(columns=["year"], inplace=True)
    assumptions_flows.drop(columns=["year"], inplace=True)
    
    # Only use storage units and flows that are needed by the user
    assumptions_sto = assumptions_sto.loc[assumptions_sto["Storage"].isin(param["technology"]["Storage"])]
    assumptions_flows = assumptions_flows.loc[assumptions_flows["Process/Storage"].isin(param["technology"]["Storage"])]
    
    # Read shapefile of processes and storage
    storage_shp = gpd.read_file(paths["process_cleaned"])
    
    # Filter out storage types not needed by the user
    storage_shp = storage_shp.loc[storage_shp["Type"].isin(param["technology"]["Storage"])]
    print("Number of storage units which belong to the modeled technologies: ", len(storage_shp), "- installed capacity: ", storage_shp["inst-cap"].sum())
    
    # Join shapefile with storage assumptions
    assumptions_sto.set_index(["Storage"], inplace=True)
    assumptions_sto.replace(to_replace="inf", value=np.inf, inplace=True)
    storage_shp = storage_shp.join(assumptions_sto, on=["Type"], how="left")
    
    # Filter out storage units that have exceeded their lifetime
    if param["model_year"] > param["year"]:
        storage_shp = storage_shp.loc[(storage_shp["lifetime"] + storage_shp["Year"]) >= param["model_year"]]
        print("Number of storage units that have not reached their end of life: ", len(storage_shp), "- installed capacity: ", storage_shp["inst-cap"].sum())

    # Associate each storage unit to a subregion
    storage_shp = get_sites(storage_shp, param)
    print("Number of storage units that lie in the modeled subregions: ", len(storage_shp), "- installed capacity: ", storage_shp["inst-cap"].sum())
    
    # Associate each storage unit to a cohort
    storage_shp["Cohort"] = (storage_shp["Year"] // param["process"]["cohorts"]) * param["process"]["cohorts"]

    # Group storage units with similar characteristics
    storage_agg = storage_shp[["Type", "inst-cap", "Cohort", "Site"]]
    storage_agg = storage_agg.groupby(["Type", "Cohort", "Site"]) \
                             .sum() \
                             .reset_index() \
                             .join(assumptions_sto, on=["Type"], how="left")
    storage_agg["Name"] = storage_agg["Type"] + "_" + storage_agg["Cohort"].astype(int).astype(str)
    print("Number of storage units after grouping: ", len(storage_agg), "- installed capacity: ", storage_agg["inst-cap"].sum())

    # Correct cap-up
    storage_agg["cap-up-p"] = storage_agg["inst-cap"].astype(float)
    
    # Clean and shorten names
    storage_agg["Name"] = storage_agg["Name"].apply(clean_names)
    
    # Obtain combinations for possible expansion
    sto_expansion = list(assumptions_sto.loc[assumptions_sto["cap-up-c"]!=0].index.unique())
    site_expansion = list(pd.read_csv(paths["sites_sub"], sep=";", decimal=",", index_col=0).index.unique())
    site_expansion = [site for site in site_expansion if not site.endswith("_offshore")]
    df_expansion = pd.DataFrame(index = pd.MultiIndex.from_product([site_expansion, sto_expansion], names=["Site", "Type"])) \
                     .reset_index() \
                     .join(assumptions_sto, on=["Type"], how="left")
    df_expansion["Cohort"] = param["model_year"]
    df_expansion["Name"] = df_expansion["Type"] + "_" + str(param["model_year"])
    df_expansion["inst-cap"] = 0
    storage_agg = storage_agg.append(df_expansion, ignore_index=True)
    print("Number of storage units after including potential expansion: ", len(storage_agg), "- installed capacity: ", storage_agg["inst-cap"].sum())
    
    # Derive efficiency and commodity from assumptions_flows
    assumptions_flows.set_index(["Process/Storage"], inplace=True)
    df_effin = assumptions_flows.loc[(assumptions_flows["Direction"]=="In") & (assumptions_flows["Commodity"]=="Elec"), "ratio"]
    df_effout = assumptions_flows.loc[(assumptions_flows["Direction"]=="Out") & (assumptions_flows["Commodity"]=="Elec"), "ratio"]
    df_com = assumptions_flows.loc[(assumptions_flows["Direction"]=="In"), "Commodity"]
    storage_agg["eff-in"] = storage_agg[["Type"]].join(df_effin, on=["Type"], how="left")["ratio"]
    storage_agg["eff-out"] = storage_agg[["Type"]].join(df_effout, on=["Type"], how="left")["ratio"]
    storage_agg["Commodity"] = storage_agg[["Type"]].join(df_com, on=["Type"], how="left")["Commodity"]   
    
    # Output
    storage_agg.to_csv(paths["storage_regions"], index=False, sep=";", decimal=",")
    print("File saved: " + paths["storage_regions"])
    create_json(paths["storage_regions"], param, ["region_name", "subregions_name", "year", "model_year", "technology", "process"], paths, ["assumptions_storage", "assumptions_flows", "process_cleaned"])
    
    timecheck("End")


# def generate_processes_and_storage_california(paths, param):
    # timecheck("Start")
    # Process = pd.read_excel(
        # paths["database_Cal"],
        # sheet_name="Operating",
        # header=1,
        # skipinitialspace=True,
        # usecols=[0, 2, 5, 6, 7, 10, 11, 14, 17, 25, 26],
        # dtype={"Entity ID": np.unicode_, "Plant ID": np.unicode_},
    # )
    # Process.rename(columns={"\nNameplate Capacity (MW)": "inst-cap", "Operating Year": "year"}, inplace=True)
    # regions = gpd.read_file(paths["regions_SHP"])

    # # Drop recently built plants (after the reference year),
    # # non-operating plants, and plants outside the geographic scope
    # Process = Process[
        # (Process["year"] <= param["year"])
        # & (Process["Status"].isin(param["pro_sto_Cal"]["status"]))
        # & (Process["Plant State"].isin(param["pro_sto_Cal"]["states"]))
    # ]

    # for i in Process.index:
        # # Define a unique ID for the processes
        # Process.loc[i, "Pro"] = (
            # Process.loc[i, "Plant State"]
            # + "_"
            # + Process.loc[i, "Entity ID"]
            # + "_"
            # + Process.loc[i, "Plant ID"]
            # + "_"
            # + Process.loc[i, "Generator ID"]
        # )

        # # Define the input commodity
        # Process.loc[i, "CoIn"] = param["pro_sto_Cal"]["proc_dict"][Process.loc[i, "Energy Source Code"]]
        # if Process.loc[i, "Technology"] == "Hydroelectric Pumped Storage":
            # Process.loc[i, "CoIn"] = "PumSt"
        # if (Process.loc[i, "CoIn"] == "Hydro_Small") and (Process.loc[i, "inst-cap"] > 30):
            # Process.loc[i, "CoIn"] = "Hydro_Large"

        # # Define the location of the process
        # if Process.loc[i, "Pro"] == "CA_50045_56284_EPG":  # Manual correction
            # Process.loc[i, "Site"] = "LAX"
        # else:
            # Process.loc[i, "Site"] = containing_polygon(Point(Process.loc[i, "Longitude"], Process.loc[i, "Latitude"]), regions)["NAME_SHORT"]

    # # Define the output commodity
    # Process["CoOut"] = "Elec"

    # # Select columns to be used
    # Process = Process[["Site", "Pro", "CoIn", "CoOut", "inst-cap", "year"]]
    # print("Number of Entries: " + str(len(Process)))

    # # Split the storages from the processes
    # process_raw = Process[~Process["CoIn"].isin(param["pro_sto_Cal"]["storage"])]
    # storage_raw = Process[Process["CoIn"].isin(param["pro_sto_Cal"]["storage"])]
    # print("Number of Processes: " + str(len(process_raw)))
    # print("Number of Storage systems: " + str(len(storage_raw)))

    # # Processes
    # # Reduce the number of processes by aggregating the small ones
    # # Select small processes and group them
    # process_group = process_raw[process_raw["inst-cap"] < 10].groupby(["Site", "CoIn"])
    # # Define the attributes of the aggregates
    # small_cap = pd.DataFrame(process_group["inst-cap"].sum())
    # small_pro = pd.DataFrame(process_group["Pro"].first() + "_agg")
    # small_coout = pd.DataFrame(process_group["CoOut"].first())
    # small_year = pd.DataFrame(process_group["year"].min())
    # # Aggregate the small processes
    # process_small = small_cap.join([small_pro, small_coout, small_year]).reset_index()

    # # Recombine big processes with the aggregated small ones
    # process_compact = process_raw[process_raw["inst-cap"] >= 10].append(process_small, ignore_index=True)
    # print("Number of Processes after agregation: " + str(len(process_compact)))
    # evrys_process, urbs_process = format_process_model_California(process_compact, process_small, param)

    # # Output
    # urbs_process.to_csv(paths["urbs_process"], index=False, sep=";", decimal=",")
    # print("File Saved: " + paths["urbs_process"])
    # evrys_process.to_csv(paths["evrys_process"], index=False, sep=";", decimal=",", encoding="ascii")
    # print("File Saved: " + paths["evrys_process"])

    # # Storage Systems
    # param["sites_evrys_unique"] = evrys_process.Sites.unique()
    # evrys_storage, urbs_storage = format_storage_model_California(storage_raw, param)

    # # Output
    # urbs_storage.to_csv(paths["urbs_storage"], index=False, sep=";", decimal=",")
    # print("File Saved: " + paths["urbs_storage"])
    # evrys_storage.to_csv(paths["evrys_storage"], index=False, sep=";", decimal=",", encoding="ascii")
    # print("File Saved: " + paths["evrys_storage"])

    # timecheck("End")
